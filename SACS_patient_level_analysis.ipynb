{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=2.1.2 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from seaborn) (3.2.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from seaborn) (0.25.3)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from seaborn) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from matplotlib>=2.1.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from pandas>=0.22.0->seaborn) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=2.1.2->seaborn) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/dilution/.pyenv/versions/3.7.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cyvcf2 import VCF\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "# prospax file collection\n",
    "df_sacs = pd.read_csv(\"SACS/SACS_all_partners.finaledit.csv\", sep=\"\\t\")\n",
    "# variant list formated\n",
    "df_sacs_extended = pd.read_csv(\"SACS/SACS.prospax.extended.annotated.tsv\", sep=\"\\t\")\n",
    "# sacs_vcf_clinvar = VCF(\"ext_data/clinvar/05_2021/SACS.clinvar.annotated.vcf\")\n",
    "df_sacs_clinvar = pd.read_csv(\"SACS/SACS.extended.annotated.tsv\", sep=\"\\t\")\n",
    "# gnomad sacs\n",
    "df_gnom_ad = pd.read_csv(\"SACS/SACS.gnomad.tsv\", sep=\"\\t\")\n",
    "# uniprot_sacs \n",
    "uniprot_var = pd.read_csv(\"SACS/SACS.uniprot.annotated.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Analysis\n",
    "\n",
    "Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change df_sacs columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"submitter_id\",\n",
    "    \"local_case_id\",\n",
    "    \"local_family_id\",\n",
    "    \"prospax_case_id\",\n",
    "    \"ngs_database_id\",\n",
    "    \"main_phenotype\",\n",
    "    \"case_status\",\n",
    "    \"id\",\n",
    "    \"gene\",\n",
    "    \"chrom\",\n",
    "    \"pos\",\n",
    "    \"ref\",\n",
    "    \"alt\",\n",
    "    \"transcript_id\",\n",
    "    \"cdna\",\n",
    "    \"prot_change\",\n",
    "    \"genotype\",\n",
    "    \"compound_het_id_s\",\n",
    "    \"paxgene_availability\",\n",
    "    \"pbmc_availability\",\n",
    "    \"fibroblasts_availability\",\n",
    "    \"comments\",\n",
    "    \"Variant_based_id\",\n",
    "]\n",
    "\n",
    "df_sacs.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify mutation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regroup_csq(x):\n",
    "    if x in [\"3_prime_UTR_variant\", \"5_prime_UTR_variant\"]:\n",
    "        return \"UTR\"\n",
    "    elif x in [\"inframe_insertion\", \"inframe_deletion\"]:\n",
    "        return \"Inframe_Indel\"\n",
    "    elif x in [\"intron_variant\" ]:\n",
    "        return \"Intron\"\n",
    "    elif x in [\"missense_variant\", \"missense_variant&splice_region_variant\"]:\n",
    "        return \"Missense\"\n",
    "    elif x in ['splice_region_variant&intron_variant',\n",
    "              'splice_region_variant&synonymous_variant', 'splice_region_variant&5_prime_UTR_variant']:\n",
    "        return \"Splice_region\"\n",
    "    elif x in [\"splice_acceptor_variant\", \"splice_donor_variant\"]:\n",
    "        return \"Canonical_splice\"\n",
    "    elif x in [\"start_lost\", \"stop_gained\", \"stop_gained&splice_region_variant\", 'stop_gained&inframe_insertion']:\n",
    "        return \"Nonsense\"\n",
    "    elif x in ['protein_altering_variant', \"frameshift_variant\", \"frameshift_variant&stop_lost\",\n",
    "               \"stop_gained&frameshift_variant\", 'splice_donor_variant&splice_acceptor_variant&frameshift_variant&stop_lost&intron_variant',\n",
    "               'frameshift_variant&splice_region_variant', 'splice_acceptor_variant&splice_donor_variant&frameshift_variant&stop_lost&intron_variant']:\n",
    "        return \"Frameshift\"\n",
    "    elif x in [\"synonymous_variant\"]:\n",
    "        return \"Synonymous\"\n",
    "    \n",
    "    \n",
    "df_sacs_clinvar[\"csq_minimal\"] = df_sacs_clinvar[\"csq\"].apply(lambda x: regroup_csq(x))\n",
    "df_sacs_extended[\"csq_minimal\"] = df_sacs_extended[\"csq\"].apply(lambda x: regroup_csq(x))\n",
    "df_gnom_ad[\"csq_minimal\"] = df_gnom_ad[\"mutation_type\"].apply(lambda x: regroup_csq(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD variant_based_id column to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacs_clinvar[\"Variant_based_id\"] = df_sacs_clinvar[[\"chrom\", \"pos\", \"ref\", \"alt\"]].apply(lambda x: \"-\".join(x.values.astype(str)), axis=1)\n",
    "df_sacs_extended[\"Variant_based_id\"] = df_sacs_extended[[\"chrom\", \"pos\", \"ref\", \"alt\"]].apply(lambda x: \"-\".join(x.values.astype(str)), axis=1)\n",
    "df_gnom_ad.rename(columns={\"variant_id\":\"Variant_based_id\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify case status \n",
    "unsure and not solved could be re-grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get variant set(ids) from different case status\n",
    "solved_ids = set(df_sacs[df_sacs['case_status']==\"solved SACS\"][\"Variant_based_id\"])\n",
    "unsure_notsolved_ids = set(df_sacs[df_sacs['case_status'].isin([\"not solved\", \"unsure\"])][\"Variant_based_id\"])\n",
    "solved_other_ids = set(df_sacs[df_sacs['case_status']==\"solved other gene\"][\"Variant_based_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD protein domain info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hgvs.parser\n",
    "import numpy as np\n",
    "\n",
    "def clean_hgvsp(x):\n",
    "    if x==\"n/a\" or x==\"-1\":\n",
    "        return \"n/a\"\n",
    "    else:\n",
    "        return x.replace(\"%3D\", \"=\")\n",
    "        \n",
    "def short_prot_desc(x):\n",
    "    if x==\"n/a\":\n",
    "        return \"n/a\"\n",
    "    else:\n",
    "        return x.split(\":\")[1]\n",
    "\n",
    "def get_prot_pos(x, hp):\n",
    "    if x==\"n/a\":\n",
    "        return -1\n",
    "    else:\n",
    "        p = hp.parse_hgvs_variant(x)\n",
    "        return p.posedit.pos.start.base\n",
    "    \n",
    "\n",
    "    \n",
    "hp = hgvs.parser.Parser()\n",
    "\n",
    "# list sacs prospax variant\n",
    "df_sacs_extended[\"hgvsp\"].fillna(\"n/a\", inplace=True)\n",
    "df_sacs_extended[\"hgvsp_clean\"] = df_sacs_extended[\"hgvsp\"].apply(lambda x: clean_hgvsp(x))\n",
    "df_sacs_extended[\"hgvsp_short\"] = df_sacs_extended[\"hgvsp_clean\"].apply(lambda x: short_prot_desc(x))\n",
    "df_sacs_extended[\"protein_pos\"] = df_sacs_extended[\"hgvsp_clean\"].apply(lambda x: get_prot_pos(x, hp))\n",
    "\n",
    "# list clinvar variant\n",
    "df_sacs_clinvar[\"hgvsp\"].fillna(\"n/a\", inplace=True)\n",
    "df_sacs_clinvar[\"hgvsp_clean\"] = df_sacs_clinvar[\"hgvsp\"].apply(lambda x: clean_hgvsp(x))\n",
    "df_sacs_clinvar[\"hgvsp_short\"] = df_sacs_clinvar[\"hgvsp_clean\"].apply(lambda x: short_prot_desc(x))\n",
    "df_sacs_clinvar[\"protein_pos\"] = df_sacs_clinvar[\"hgvsp_clean\"].apply(lambda x: get_prot_pos(x, hp))\n",
    "\n",
    "# list gnomad variant\n",
    "df_gnom_ad[\"hgvsp\"].fillna(\"n/a\", inplace=True)\n",
    "df_gnom_ad[\"hgvsp_clean\"] = df_gnom_ad[\"hgvsp\"].apply(lambda x: clean_hgvsp(x))\n",
    "df_gnom_ad[\"hgvsp_short\"] = df_gnom_ad[\"hgvsp_clean\"].apply(lambda x: short_prot_desc(x))\n",
    "df_gnom_ad[\"protein_pos\"] = df_gnom_ad[\"hgvsp_clean\"].apply(lambda x: get_prot_pos(x, hp))\n",
    "\n",
    "\n",
    "# list gnomad variant\n",
    "uniprot_var[\"hgvsp\"].fillna(\"n/a\", inplace=True)\n",
    "uniprot_var[\"hgvsp_clean\"] = uniprot_var[\"hgvsp\"].apply(lambda x: clean_hgvsp(x))\n",
    "uniprot_var[\"hgvsp_short\"] = uniprot_var[\"hgvsp_clean\"].apply(lambda x: short_prot_desc(x))\n",
    "uniprot_var[\"protein_pos\"] = uniprot_var[\"hgvsp_clean\"].apply(lambda x: get_prot_pos(x, hp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacs_domain = pd.read_csv(\"SACS/sacs.bed\", sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "# map of regions\n",
    "sacs_prot = {\n",
    "    \"0_inter\": (1, 8),\n",
    "    \"1_ubq_like\": (9, 83),\n",
    "    \"2_sr1_SIRPT1\": (84, 339),\n",
    "    \"3_SRR1\": (340, 399),\n",
    "    \"4_sr2_SIRPT1\": (400, 557),\n",
    "    \"5_inter\": (558, 643),\n",
    "    \"6_srX_SIRPT1\": (644, 1162),\n",
    "    \"7_inter\": (1163, 1220),\n",
    "    \"8_sr3_SIRPT1\": (1221, 1374),\n",
    "    \"9_interinter\": (1375, 1443),\n",
    "    \"10_sr1_SIRPT2\": (1444, 1747),\n",
    "    \"11_SRR2\": (1748, 1825),\n",
    "    \"12_sr2_SIRPT2\": (1826, 1968),\n",
    "    \"13_inter\": (1969, 2286),\n",
    "    \"14_sr3_SIRPT3\": (2287, 2443),\n",
    "    \"15_inter\": (2444, 2511),\n",
    "    \"16_sr1_SIRPT3\": (2512, 2768),\n",
    "    \"17_SRR3\": (2769, 2825),\n",
    "    \"18_sr2_SIRPT3\": (2826, 2960),\n",
    "    \"19_inter\": (2961, 3080),\n",
    "    \"20_srX_SIRPT3\": (3081, 3659),\n",
    "    \"21_inter\": (3660, 3735),\n",
    "    \"22_sr3_SIRPT3\": (3736, 3896),\n",
    "    \"23_inter\": (3897, 4305),\n",
    "    \"24_dnaj\": (4306, 4393),\n",
    "    \"25_inter\": (4394, 4450),\n",
    "    \"26_hepn\": (4451, 4567),\n",
    "    \"27_inter\": (4568, 4600)\n",
    "}\n",
    "\n",
    "# reduce to categories\n",
    "ref = {\n",
    "    \"0_inter\": (0, [0, 5, 7, 9, 13, 15, 19, 21, 23, 25, 27]),\n",
    "    \"1_ubq_like\": (1, [1]),\n",
    "    \"2_sr1_histidine_kinase\": (2, [2, 10, 16]),\n",
    "    \"3_sr2\": (3, [4, 12, 18]),\n",
    "    \"4_SRR\": (4, [3, 11, 17]),\n",
    "    \"5_srx\": (5, [6, 20]),\n",
    "    \"6_sr3\": (6, [8, 14, 22]),\n",
    "    \"7_dnaj\": (7, [24]),\n",
    "    \"8_hepn\": (8, [26])\n",
    "}\n",
    "\n",
    "def get_domain_name(x, map_dom):\n",
    "    \n",
    "    if x==-1:\n",
    "        return -1\n",
    "    else:\n",
    "        intervals = sorted([v[1] for i, v in map_dom.items()])\n",
    "        intervals_name = [i for i in map_dom]\n",
    "        domain = bisect.bisect_left(intervals, x)\n",
    "        domain_name = intervals_name[domain]\n",
    "        return domain_name\n",
    "\n",
    "def get_domain_cat(x, map_dom, ref):\n",
    "    if x==-1:\n",
    "        return -1\n",
    "    else:\n",
    "        intervals = sorted([v[1] for i, v in map_dom.items()])\n",
    "        # intervals_name = [i for i in map_dom]\n",
    "        domain = bisect.bisect_left(intervals, x)\n",
    "        for i, v in ref.items():\n",
    "            if domain in v[1]:\n",
    "                return v[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to df_var\n",
    "df_sacs_extended[\"domain_name\"] = df_sacs_extended[\"protein_pos\"].apply(lambda x: get_domain_name(x, sacs_prot))\n",
    "df_sacs_extended[\"cat_domain\"] = df_sacs_extended[\"protein_pos\"].apply(lambda x: get_domain_cat(x, sacs_prot, ref))\n",
    "\n",
    "# clinvar\n",
    "df_sacs_clinvar[\"domain_name\"] = df_sacs_clinvar[\"protein_pos\"].apply(lambda x: get_domain_name(x, sacs_prot))\n",
    "df_sacs_clinvar[\"cat_domain\"] = df_sacs_clinvar[\"protein_pos\"].apply(lambda x: get_domain_cat(x, sacs_prot, ref))\n",
    "\n",
    "\n",
    "# gnomad\n",
    "df_gnom_ad[\"domain_name\"] = df_gnom_ad[\"protein_pos\"].apply(lambda x: get_domain_name(x, sacs_prot))\n",
    "df_gnom_ad[\"cat_domain\"] = df_gnom_ad[\"protein_pos\"].apply(lambda x: get_domain_cat(x, sacs_prot, ref))\n",
    "\n",
    "# uniprot\n",
    "uniprot_var[\"domain_name\"] = uniprot_var[\"protein_pos\"].apply(lambda x: get_domain_name(x, sacs_prot))\n",
    "uniprot_var[\"cat_domain\"] = uniprot_var[\"protein_pos\"].apply(lambda x: get_domain_cat(x, sacs_prot, ref))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparatory steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Annotate variants uniformely --> (done with VEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Identify and flag likely pathogenic variants\n",
    "- All rare truncating variants (nonsense, frameshift, canonical splice)\n",
    "- All ClinVar likely pathogenic / pathogenic variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_lp_p(x, list_v_id):\n",
    "    if x in list_v_id:\n",
    "        return \"y\"\n",
    "    else:\n",
    "        return \"n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get truncating mutation \n",
    "truncating_type = ['Nonsense', 'Frameshift', 'Canonical_splice']\n",
    "list_v_id_truncating = df_sacs_extended[df_sacs_extended[\"csq_minimal\"].isin(truncating_type)][\"Variant_based_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get likely pathogenic and pathogenic from clinvar and uniprot\n",
    "# inner merge clinvar prospax\n",
    "# assertion considered are: \n",
    "# 'reviewed_by_expert_panel',\n",
    "# 'criteria_provided,_single_submitter',\n",
    "# 'criteria_provided,_multiple_submitters,_no_conflicts'\n",
    "inner_merge_clinvar_prospax = pd.merge(df_sacs_clinvar[[\"Variant_based_id\", 'clinsig', 'clinsigconflict','clinsigincl']], df_sacs_extended, how=\"inner\", on=[\"Variant_based_id\"])\n",
    "lp_p_labels = ['Likely_pathogenic', 'Pathogenic/Likely_pathogenic', 'Pathogenic']\n",
    "lp_p_clinvar = inner_merge_clinvar_prospax[inner_merge_clinvar_prospax[\"clinsig\"].isin(lp_p_labels)][\"Variant_based_id\"].unique()\n",
    "\n",
    "# inner merge uniprot prospax \n",
    "# pathogenic are labeled as 2 in clinsig col\n",
    "inner_merge_uniprot_prospax = pd.merge(uniprot_var[[\"Variant_based_id\", 'clinsig']], df_sacs_extended, how=\"inner\", on=[\"Variant_based_id\"])\n",
    "lp_p_uniprot = inner_merge_uniprot_prospax[inner_merge_uniprot_prospax[\"clinsig\"]==2][\"Variant_based_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gross indels to list of lp_p variants\n",
    "gross_indels = list(df_sacs[df_sacs[\"ref\"].isin([\"dup\", \"del\"])][\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge lists lp/p uniprot / clinvar / truncating variants\n",
    "# and flag them in variant collection\n",
    "final_lp_p_id = list(set(list(lp_p_clinvar) + list(lp_p_uniprot) + list(list_v_id_truncating) + gross_indels))\n",
    "df_sacs_extended[\"is_lp_p\"] = df_sacs_extended[\"Variant_based_id\"].apply(lambda x: flag_lp_p(x, final_lp_p_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants being reported as LP/P in clinvar/uniprot: 125\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "print(\"Number of variants being reported as LP/P in clinvar/uniprot: {}\".format(df_sacs_extended[df_sacs_extended[\"is_lp_p\"]==\"y\"].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants being reported as LP/P in clinvar/uniprot + gross deletions: 129\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variants being reported as LP/P in clinvar/uniprot + gross deletions: {}\".format(len(final_lp_p_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Filter remaining variants uniformely\n",
    "- Keep all likely pathogenic variants\n",
    "- Filter criteria for remaining variants: \n",
    "    - MAF < 0.5%\n",
    "    - highly conserved (missense only: PhasCons ≥ 0.7, GERP ≥ 2) (to be discussed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_maf(df, threshold=0.005):\n",
    "    df_filt = df[(df['thKg']<=threshold) &\n",
    "                 (df['gnomad_ex']<=threshold) &\n",
    "                 (df['gnomad_gn']<=threshold)\n",
    "                ]\n",
    "    return df_filt\n",
    "\n",
    "def filter_conservation(df, phastcons_min=0.7, gerp_min=2):\n",
    "    df_filt = df[((df['gerp_rs']>=gerp_min) | (df['gerp_rs'].isnull())) &\n",
    "                 ((df['phastcons']>=phastcons_min) | (df['phastcons'].isnull())) \n",
    "                ]\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(269, 55)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_not_lp_p = df_sacs_extended[df_sacs_extended[\"is_lp_p\"]==\"n\"]\n",
    "df_not_lp_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the non-LP/P variant from preparatory steps\n",
    "# filter on MAF<=0.5%\n",
    "df_freq1 = filter_maf(df_not_lp_p)\n",
    "df_freq_conserver_filtered = filter_conservation(df_freq1)\n",
    "final_conserved_variant_id = list(df_freq_conserver_filtered[\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag variants if lp/p - conserved - none\n",
    "def flag_conserved(x, list_lp_p, list_rare_conserved):\n",
    "    if x in list_rare_conserved:\n",
    "        return \"rare_conserved\"\n",
    "    elif x in list_lp_p:\n",
    "        return \"lp_p\"\n",
    "    else:\n",
    "        return \"not_conserved\"\n",
    "    \n",
    "    \n",
    "df_sacs_extended[\"is_conserved\"] = df_sacs_extended[\"Variant_based_id\"].apply(lambda x: flag_conserved(x, final_lp_p_id, final_conserved_variant_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 55)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq_conserver_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - Tier 1: Cases carrying 2 likely pathogenic variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Identify cases carrying two (likely) pathogenic variants\n",
    "- flag cases as “solved”\n",
    "- filter out all variants in solved cases\n",
    "- collect phenotypic information, perform segregation analysis, consider functional assays (e.g. as positive controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique patient (submitter-localsubjectid)\n",
    "df_lp_p = df_sacs[df_sacs[\"Variant_based_id\"].isin(final_lp_p_id)]\n",
    "\n",
    "unique_case_list = set([(row['submitter_id'],row['local_case_id']) \n",
    "                    for idx, row in df_lp_p.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_cases(cases, df):\n",
    "    for case in cases:\n",
    "        yield((case, df[(df['local_case_id']==case[1]) & \\\n",
    "                 (df['submitter_id']==case[0])]))\n",
    "\n",
    "# check if patient has one gross dup/del        \n",
    "def is_large_dup_del(df):\n",
    "    if \"dup\" in df['ref'].values:\n",
    "        return True\n",
    "    elif \"del\" in df['ref'].values:\n",
    "        return True\n",
    "    else:\n",
    "        False\n",
    "        \n",
    "# check if patient has 2 lp/p variants       \n",
    "def is_2_lp_p(df, list_lp_p):\n",
    "    if df[df[\"Variant_based_id\"].isin(list_lp_p)].shape[0]>=2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# check if patient has 1 lp/p variant + one gross dup/del\n",
    "def is_lpp_del_dup(df, list_lp_p):\n",
    "    if df[df[\"Variant_based_id\"].isin(list_lp_p)].shape[0]>1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# check if patient has Hom lp/p variant\n",
    "def is_lp_p_hom(df, list_lp_p):\n",
    "    if df[(df[\"Variant_based_id\"].isin(list_lp_p)) &\n",
    "       (df[\"genotype\"]==\"Hom\")].empty:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def tier1_screen_case(df, list_lp_p):\n",
    "    # if one variant from lp/p list only variant hom\n",
    "    if df.shape[0]==1:\n",
    "        return True\n",
    "    # if more than one variant in case\n",
    "    elif df.shape[0]>1:\n",
    "        # if on variant is a gross del/dup\n",
    "        if is_large_dup_del(df):\n",
    "            # if one gross dup/del + one lp/p variant\n",
    "            if is_lpp_del_dup(df, list_lp_p):\n",
    "                return True\n",
    "            else:\n",
    "                # if just one gross dup/del\n",
    "                return False\n",
    "        # if 2 lp/p het / caveats ID27288 + 357-987-169\n",
    "        elif is_2_lp_p(df, list_lp_p):\n",
    "            return True\n",
    "        # if one HOM lp/p variants and other non het/HOM non-lp/p variants\n",
    "        elif is_lp_p_hom(df, list_lp_p):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_dup_del_lp = df_sacs[df_sacs[\"local_case_id\"]==\"ARSACS11\"]    \n",
    "test_2_lp_p = df_sacs[df_sacs[\"local_case_id\"]==\"ARSACS9\"]  \n",
    "test_not_2_lp = df_sacs[df_sacs[\"local_case_id\"]==\"P3_ID7\"]\n",
    "test_dup_del_no_lp = df_sacs[df_sacs[\"local_case_id\"]==\"156-038-303\"]\n",
    "test_random = df_sacs[df_sacs[\"local_case_id\"]==\"FN14062\"]\n",
    "test_hom_many = df_sacs[df_sacs[\"local_case_id\"]==\"SCA1511\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true\n",
    "tier1_screen_case(test_dup_del_lp, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true\n",
    "tier1_screen_case(test_2_lp_p, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false\n",
    "tier1_screen_case(test_not_2_lp, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false\n",
    "tier1_screen_case(test_dup_del_no_lp, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false\n",
    "tier1_screen_case(test_random, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true\n",
    "tier1_screen_case(test_hom_many, final_lp_p_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collect case local id having 2 lp/p variants or 1 lp/p variant + gross del/dup\n",
    "solved_cases = []\n",
    "for case in stream_cases(unique_case_list, df_sacs):\n",
    "    patient = case[0]\n",
    "    df = case[1]\n",
    "    if tier1_screen_case(df, final_lp_p_id):\n",
    "        solved_cases.append(patient)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cases from prospax collection\n",
    "# flag them in df_sacs\n",
    "def is_solved(x, solved_id):\n",
    "    if x in solved_id:\n",
    "        return \"y\"\n",
    "    else:\n",
    "        return \"n\"\n",
    "solved_local_ids = {case[1] for case in solved_cases}\n",
    "df_sacs[\"tier1_is_solved\"] = df_sacs[\"local_case_id\"].apply(lambda x: is_solved(x, solved_local_ids))\n",
    "df_solved = df_sacs[df_sacs[\"local_case_id\"].isin(solved_local_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cases being tier1: 152\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique cases being tier1: {}\".format(df_solved[\"local_case_id\"].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cases being tier 1\n",
    "df_solved.to_csv(\"SACS/tier1/tier1_solved_case.tsv\", sep=\"\\t\", index=False)\n",
    "df_solved.to_excel(\"SACS/tier1/tier1_solved_case.xlsx\", index=False, freeze_panes=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract variant list being lp/p and Tier1\n",
    "tiers1_variant_id = df_solved[df_solved[\"Variant_based_id\"].isin(final_lp_p_id)][\"Variant_based_id\"].unique()\n",
    "tiers1_variants = df_sacs_extended[df_sacs_extended[\"Variant_based_id\"].isin(tiers1_variant_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tier1: lp-p variants \n",
    "tiers1_variants.to_csv(\"SACS/tier1/tiers_solved_variants.tsv\", sep=\"\\t\", index=False)\n",
    "tiers1_variants.to_excel(\"SACS/tier1/tiers_solved_variants.xlsx\", index=False, freeze_panes=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of solved cases\n",
    "len(df_solved[\"local_case_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_solved[\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any benign among Tier1??\n",
    "[v for v in tiers1_variants[\"Variant_based_id\"].unique() if v in final_list_benign]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - Tier 2: Cases carrying 1 likely pathogenic variants in combination with a VUS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 -  Identify cases carrying 1 likely pathogenic variant in combination with at least 1 additional variant\n",
    "- depending on the number of remaining cases, proceed to functional follow up or consider additional filtering steps\n",
    "- criteria supporting pathogenicity: \n",
    "    - phenotype matches SACS/SPG7\n",
    "    - variants segregate in the family\n",
    "    - highly conserved, predicted deleterious (to be discussed)\n",
    "    - missense variant is located in a functional domain\n",
    "    - variant has functional effect (splicing, downregulation of protein, downstream functional effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522, 24)\n",
      "(34, 24)\n"
     ]
    }
   ],
   "source": [
    "# create unique patient (submitter-localsubjectid)\n",
    "df_tier2_prep = df_sacs[df_sacs[\"tier1_is_solved\"]==\"n\"]\n",
    "print(df_tier2_prep.shape)\n",
    "df_tier2_prep = df_tier2_prep[df_tier2_prep[\"Variant_based_id\"].isin(final_lp_p_id)]\n",
    "print(df_tier2_prep.shape)\n",
    "tier2_unique_case_list = set([(row['submitter_id'],row['local_case_id']) \n",
    "                    for idx, row in df_tier2_prep.iterrows()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** remember to use non-lp/p conserved&rare variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_conserved_variants = df_freq_conserver_filtered[\"Variant_based_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** create list of benign-likelybenign variants to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_clinvar = list(df_sacs_clinvar[df_sacs_clinvar[\"clinsig\"].isin(['Likely_benign', 'Benign', 'Benign/Likely_benign'])][\"Variant_based_id\"].unique())\n",
    "\n",
    "# remove 2 uniprot variants which are VUS\n",
    "benign_uniprot = list(uniprot_var[uniprot_var[\"clinsig\"]==1][\"Variant_based_id\"].unique())\n",
    "benign_uniprot.remove(\"13-23906766-T-C\")\n",
    "benign_uniprot.remove(\"13-23912630-C-T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_benign = list(set(benign_clinvar + benign_uniprot))\n",
    "# add flag in main variant collection - is_variant_benign reported\n",
    "def is_benign_uniprot_clinvar(x, list_benign):\n",
    "    if x in list_benign:\n",
    "        return \"y\"\n",
    "    else:\n",
    "        return \"n\"\n",
    "    \n",
    "df_sacs_extended[\"has_benign_report\"] = df_sacs_extended[\"Variant_based_id\"].apply(lambda x: is_benign_uniprot_clinvar(x, final_list_benign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stream_cases(cases, df):\n",
    "    for case in cases:\n",
    "        yield((case, df[(df['local_case_id']==case[1]) & \\\n",
    "                 (df['submitter_id']==case[0])]))\n",
    "        \n",
    "def get_lp_p_variant(df, list_lp_p):\n",
    "    list_df_lp_p = list(df[df[\"Variant_based_id\"].isin(list_lp_p)][\"Variant_based_id\"])\n",
    "    return list_df_lp_p\n",
    "\n",
    "def get_rare_conserved_variant(df, list_conserved_rare):\n",
    "    list_df_conserverd_rare = list(df[df[\"Variant_based_id\"].isin(list_conserved_rare)][\"Variant_based_id\"])\n",
    "    return list_df_conserverd_rare\n",
    "\n",
    "def get_benign_variant(df, list_benign):\n",
    "    list_df_benign = list(df[df[\"Variant_based_id\"].isin(list_benign)][\"Variant_based_id\"])\n",
    "    return list_df_benign\n",
    "\n",
    "def get_all_var_not_lpp(df, v_lp_p):\n",
    "    v_not_lpp = [v for v in df[\"Variant_based_id\"].unique() if v not in v_lp_p]\n",
    "    return v_not_lpp\n",
    "\n",
    "def get_hyp_vus(v_not_lpp, v_benign, v_rare_conserved):\n",
    "    hyp_vus = [v for v in v_not_lpp if v not in v_benign and v in v_rare_conserved]\n",
    "    return hyp_vus\n",
    "\n",
    "def is_tier2(df, df_length, v_lp_p, v_benign, v_rare_conserved):\n",
    "    if df_length==2:\n",
    "        if len(v_lp_p)==1 and len(v_benign)==0 and len(v_rare_conserved)==1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        not_lpp_v = get_all_var_not_lpp(df, v_lp_p)\n",
    "        hyp_vus = get_hyp_vus(not_lpp_v, v_benign, v_rare_conserved)\n",
    "        if hyp_vus:\n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "        \n",
    "def tier2_screen_case(df, list_lp_p, list_conserved_rare, list_benign):\n",
    "    if df.shape[0]==1:\n",
    "        print(\"problem 1 variant only\", list(df[\"Variant_based_id\"]))\n",
    "    else:\n",
    "        v_lp_p = get_lp_p_variant(df, list_lp_p)\n",
    "        # print(\"lp_p\", len(v_lp_p))\n",
    "        v_benign = get_benign_variant(df, list_benign)\n",
    "        # print(\"benign\", len(v_benign))\n",
    "        v_rare_conserved = get_rare_conserved_variant(df, list_conserved_rare)\n",
    "        # print(\"conserved_rare\", len(v_rare_conserved))\n",
    "        df_length = df.shape[0]\n",
    "        \n",
    "        return is_tier2(df, df_length, v_lp_p, v_benign, v_rare_conserved)\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pisa', '279-716-956') not tier2\n",
      "('P3 - Nijmegen', 'P3_ID28') not tier2\n",
      "('P6 - Istanbul', 'MYO47') not tier2\n"
     ]
    }
   ],
   "source": [
    "tier2_cases = []\n",
    "tier2_cases_rejected = []\n",
    "for case in stream_cases(tier2_unique_case_list, df_sacs):\n",
    "    patient = case[0]\n",
    "    df = case[1]\n",
    "    #if patient[1]==\"ATX81\":\n",
    "        #print(df.shape)\n",
    "        \n",
    "        #print(tier2_screen_case(df, final_lp_p_id, final_conserved_variant_id, final_list_benign))\n",
    "    if tier2_screen_case(df, final_lp_p_id, final_conserved_variant_id, final_list_benign):\n",
    "        tier2_cases.append(patient)\n",
    "    else:\n",
    "        tier2_cases_rejected.append(patient)\n",
    "        print(patient, \"not tier2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# extract cases from prospax collection\n",
    "# flag them in df_sacs\n",
    "def flag_tier2(x, tier2_id, tier2_cases_rejected):\n",
    "    if x in tier2_id:\n",
    "        return \"y\"\n",
    "    elif x in tier2_cases_rejected:\n",
    "        return \"rejected_automatic_not_rare/conserved_or_benign_report\"\n",
    "    else:\n",
    "        return \"n\"\n",
    "\n",
    "def flag_tier2_variant_lvl(x, lp_p, rare_conserved, benign):\n",
    "    if x in lp_p:\n",
    "        return \"lp_p_variant\"\n",
    "    elif x in rare_conserved and x not in benign:\n",
    "        return \"rare_conserved_variant\"\n",
    "    elif x not in rare_conserved and x not in benign:\n",
    "        return \"NOT_rare_conserved_variant\"\n",
    "    elif x not in rare_conserved and x in benign:\n",
    "        return \"NOT_rare_conserved_variant_AND_benign_report\"\n",
    "    \n",
    "tier2_local_ids = {case[1] for case in tier2_cases}\n",
    "tier2_rejected_local_ids = {case[1] for case in tier2_cases_rejected}\n",
    "df_sacs[\"is_tier2\"] = df_sacs[\"local_case_id\"].apply(lambda x: flag_tier2(x, tier2_local_ids, tier2_rejected_local_ids))\n",
    "df_tier2 = df_sacs[df_sacs[\"local_case_id\"].isin(tier2_local_ids)]\n",
    "df_tier2[\"variant_level_status\"] = df_tier2[\"Variant_based_id\"].apply(lambda x: flag_tier2_variant_lvl(x, final_lp_p_id, final_conserved_variant_id, final_list_benign))\n",
    "df_tier2_rejected = df_sacs[df_sacs[\"local_case_id\"].isin(tier2_rejected_local_ids)]\n",
    "df_tier2_rejected[\"variant_level_status\"] = df_tier2_rejected[\"Variant_based_id\"].apply(lambda x: flag_tier2_variant_lvl(x, final_lp_p_id, final_conserved_variant_id, final_list_benign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# create column to flag questionable variants \n",
    "def flag_questionable(x):\n",
    "    if x == \"lp_p_variant\":\n",
    "        return 0\n",
    "    elif x == \"rare_conserved_variant\":\n",
    "        return 2\n",
    "    elif x in [\"NOT_rare_conserved_variant\", \"NOT_rare_conserved_variant_AND_benign_report\"]:\n",
    "        return 1\n",
    "    \n",
    "df_tier2[\"is_questionable\"] = df_tier2[\"variant_level_status\"].apply(lambda x: flag_questionable(x))\n",
    "df_tier2_rejected[\"is_questionable\"] = df_tier2_rejected[\"variant_level_status\"].apply(lambda x: flag_questionable(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tier2_variants = df_sacs_extended[df_sacs_extended[\"Variant_based_id\"].isin(df_tier2[\"Variant_based_id\"].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def flag_questionable_var(x):\n",
    "    if x == \"lp_p\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df_tier2_variants[\"is_questionable_variant\"] = df_tier2_variants[\"is_conserved\"].apply(lambda x: flag_questionable_var(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tier2_variants.to_csv(\"SACS/tier2/tiers2_cases_variants.tsv\", sep=\"\\t\", index=False)\n",
    "df_tier2_variants.to_excel(\"SACS/tier2/tiers2_cases_variants.xlsx\", index=False, freeze_panes=(1,4))\n",
    "df_tier2.to_csv(\"SACS/tier2/tier2_cases.tsv\", sep=\"\\t\", index=False)\n",
    "df_tier2.to_excel(\"SACS/tier2/tier2_cases.xlsx\", index=False, freeze_panes=(1,4))\n",
    "df_tier2_rejected.to_csv(\"SACS/tier2/tier2_rejected_cases.tsv\", sep=\"\\t\", index=False)\n",
    "df_tier2_rejected.to_excel(\"SACS/tier2/tier2_rejected_cases.xlsx\", index=False, freeze_panes=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tier2[\"local_case_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False\n",
    "test_one_lp_benign = df_sacs[df_sacs[\"local_case_id\"]==\"MYO47\"]\n",
    "tier2_screen_case(test_one_lp_benign, final_lp_p_id, final_conserved_variant_id, final_list_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True\n",
    "test_true = df_sacs[df_sacs[\"local_case_id\"]==\"AAR-BOR-CLA-334\"]\n",
    "tier2_screen_case(test_true, final_lp_p_id, final_conserved_variant_id, final_list_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True \n",
    "test_complex = df_sacs[df_sacs[\"local_case_id\"]==\"ATX81\"]\n",
    "tier2_screen_case(test_complex, final_lp_p_id, final_conserved_variant_id, final_list_benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C - Tier 3: Cases carrying 2 VUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Identify cases carrying 2 variants of unknown significance\n",
    "- apply standard filters proposed by Robin (see next slide)\n",
    "- depending on the number of remaining cases, proceed to functional follow up or consider additional filtering steps\n",
    "- criteria supporting pathogenicity: \n",
    "    - phenotype matches SACS/SPG7\n",
    "    - variants segregate in the family\n",
    "    - missense variant is located in a functional domain\n",
    "    - variant has functional effect (splicing, downregulation of protein, downstream functional effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases to investigate in the context of Tier3 analysis: 177\n"
     ]
    }
   ],
   "source": [
    "# get cases which are not included in tier1 and tier2\n",
    "df_tier3_prep = df_sacs[(df_sacs[\"tier1_is_solved\"]==\"n\") & (df_sacs[\"is_tier2\"]==\"n\")]\n",
    "\n",
    "print(\"Number of cases to investigate in the context of Tier3 analysis: {}\".format(len(df_tier3_prep[\"local_case_id\"].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases to investigate\n",
    "tier3_unique_case_list = df_tier3_prep[\"local_case_id\"].unique()\n",
    "# get variants id tier3\n",
    "variants_tier3_unique_cases = df_tier3_prep[\"Variant_based_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variants to investigate in the context of Tier3 analysis: 238\n"
     ]
    }
   ],
   "source": [
    "# get variants from tier3_unique_case_list\n",
    "df_pre_tier3_variants = df_sacs_extended[df_sacs_extended[\"Variant_based_id\"].isin(variants_tier3_unique_cases)]\n",
    "\n",
    "print(\"Number of variants to investigate in the context of Tier3 analysis: {}\".format(df_pre_tier3_variants.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate variants according to mutation type\n",
    "# exonic / no synonymous / no splice variants region / checked they are all intronic atm\n",
    "exonic = df_pre_tier3_variants[df_pre_tier3_variants[\"csq_minimal\"].isin([\"Missense\", \"Inframe_Indel\"])]\n",
    "\n",
    "# missense\n",
    "missense_pre_tier3 = df_pre_tier3_variants[df_pre_tier3_variants[\"csq_minimal\"]==\"Missense\"]\n",
    "\n",
    "# inframe indel\n",
    "inframe_indels_tier3 = df_pre_tier3_variants[df_pre_tier3_variants[\"csq_minimal\"]==\"Inframe_Indel\"]\n",
    "\n",
    "# untranslated_region\n",
    "untranslated_regions_tier3 = df_pre_tier3_variants[df_pre_tier3_variants[\"csq_minimal\"].isin(['UTR', 'Intron', 'Splice_region'])]\n",
    "\n",
    "# synonymous\n",
    "synonymous = df_pre_tier3_variants[df_pre_tier3_variants[\"csq_minimal\"]==\"Synonymous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exonic[\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply filtration to whole pre_tier3 collection (considering all mutation type)\n",
    "- two filtration:\n",
    "    - deleteriouness \n",
    "    - spliceai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filtration \n",
    "def filter_tier3(df):\n",
    "    df = df[((df['metalr_pred'] == \"D\") | (df['metalr_pred'].isnull())) &\n",
    "           ((df['m_cap_pred'] == \"D\") | (df['m_cap_pred'].isnull())) &\n",
    "           ((df['sift']==\"deleterious\") | (df['sift'].isnull())) &\n",
    "           ((df['polyphen'].isin([\"probably_damaging\", \"possibly_damaging\"]))  | (df['polyphen'].isnull())) &\n",
    "           ((df['mutationtaster_pred']=='D&D&N') | (df['mutationtaster_pred']=='D&D&D') | (df['mutationtaster_pred']=='A&A&D') | (df['mutationtaster_pred'].isnull())) &\n",
    "           ((df['cadd']>=(15)) | (df['cadd'].isnull())) & \n",
    "           ((df['dann']>=(0.98)) | (df['dann'].isnull())) &\n",
    "           ((df['revel']>=(0.5)) | (df['revel'].isnull())) &\n",
    "           ((df['provean_pred'].isin([\"D&D\", \"D&N\", \"D\", \"N&D\"])) | (df['provean_pred'].isnull()))]\n",
    "    return df\n",
    "\n",
    "def filter_spliceai(df, min_score=0.2):\n",
    "    df = df[\n",
    "            (df['spliceai_pred_DS_AG']>=min_score) |\n",
    "            (df['spliceai_pred_DS_AL']>=min_score) |\n",
    "            (df['spliceai_pred_DS_DG']>=min_score) |\n",
    "            (df['spliceai_pred_DS_DL']>=min_score) \n",
    "    ]\n",
    "    return df\n",
    "\n",
    "# main filtration\n",
    "df_filtered_pre_tier3 = filter_tier3(exonic)\n",
    "    \n",
    "# spliceai filtration\n",
    "df_splice_filtered_pre_tier3 = filter_spliceai(df_pre_tier3_variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_splice_filtered_pre_tier3[\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge splice and filtered variants filtered\n",
    "df_filtered_tier3_stage1 = pd.concat([df_filtered_pre_tier3, df_splice_filtered_pre_tier3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dilution/.pyenv/versions/3.7.7/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# from filtered list fetch cases with variants\n",
    "filtered_variants_tiers3 = df_filtered_tier3_stage1[\"Variant_based_id\"].unique()\n",
    "# get cases id\n",
    "cases_filtered_tier3_stage1 = df_tier3_prep[df_tier3_prep[\"Variant_based_id\"].isin(filtered_variants_tiers3)][\"local_case_id\"].unique()\n",
    "# get df\n",
    "df_tier3_cases_stage1 = df_tier3_prep[df_tier3_prep[\"local_case_id\"].isin(cases_filtered_tier3_stage1)]\n",
    "\n",
    "# did variant pass filtration?\n",
    "def flag_variant_filter(x, pass_v_l):\n",
    "    if x in pass_v_l:\n",
    "        return \"y\"\n",
    "    else: \n",
    "        return \"n\"\n",
    "    \n",
    "df_tier3_cases_stage1[\"pass_tier3_filtration\"] = df_tier3_cases_stage1[\"Variant_based_id\"].apply(lambda x: flag_variant_filter(x, filtered_variants_tiers3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variants passing filtration\n",
    "df_filtered_tier3_stage1.to_csv(\"SACS/tier3/stage1_tier3_variants.tsv\", sep=\"\\t\", index=False)\n",
    "df_filtered_tier3_stage1.to_excel(\"SACS/tier3/stage1_tier3_variants.xlsx\", index=False, freeze_panes=(1,4))\n",
    "# save cases carrying variant\n",
    "df_tier3_cases_stage1.to_csv(\"SACS/tier3/stage1_tier3_cases.tsv\", sep=\"\\t\", index=False)\n",
    "df_tier3_cases_stage1.to_excel(\"SACS/tier3/stage1_tier3_cases.xlsx\", index=False, freeze_panes=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tier3_cases_stage1[\"local_case_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_filtered_tier3_stage1[\"Variant_based_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submitter_id</th>\n",
       "      <th>local_case_id</th>\n",
       "      <th>local_family_id</th>\n",
       "      <th>prospax_case_id</th>\n",
       "      <th>ngs_database_id</th>\n",
       "      <th>main_phenotype</th>\n",
       "      <th>case_status</th>\n",
       "      <th>id</th>\n",
       "      <th>gene</th>\n",
       "      <th>chrom</th>\n",
       "      <th>...</th>\n",
       "      <th>prot_change</th>\n",
       "      <th>genotype</th>\n",
       "      <th>compound_het_id_s</th>\n",
       "      <th>paxgene_availability</th>\n",
       "      <th>pbmc_availability</th>\n",
       "      <th>fibroblasts_availability</th>\n",
       "      <th>comments</th>\n",
       "      <th>Variant_based_id</th>\n",
       "      <th>tier1_is_solved</th>\n",
       "      <th>is_tier2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID16341</td>\n",
       "      <td>FN16341</td>\n",
       "      <td>064-258-853</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_1</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Gly1275Ter</td>\n",
       "      <td>Het</td>\n",
       "      <td>P1_SACS_Tubingen_2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23914246-C-A</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID23075</td>\n",
       "      <td>FN23075</td>\n",
       "      <td>167-759-767</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_3</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Arg1575Trp</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23913292-G-A</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID24107</td>\n",
       "      <td>FN23075</td>\n",
       "      <td>444-749-699</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_4</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Arg1575Trp</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23913292-G-A</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID27288</td>\n",
       "      <td>FN27286</td>\n",
       "      <td>493-515-735</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_5</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Thr2388Argfs*10</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23910851-GGT-G</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ATX553</td>\n",
       "      <td>ATX553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_28</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Arg3903Ter</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23906308-G-A</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-STR-CHA-314-1</td>\n",
       "      <td>AAR-STR-CHA-314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_84</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Asp202Gly</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_85</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23930146-T-C</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-STR-CHA-314-2</td>\n",
       "      <td>AAR-STR-CHA-314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_86</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Asp202Gly</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_87</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23930146-T-C</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-SAL-GOL-61-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_88</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Trp144ValfsTer39</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_89</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23939332-AAA-A</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAD-LYO-BUF-28-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_90</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Leu96ThrfsTer3</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_91</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23942600-G-GT</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-SAL-ROD-819-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_92</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>p.Arg742Ter</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23915791-G-A</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submitter_id      local_case_id  local_family_id prospax_case_id  \\\n",
       "0    P1 - Tübingen            ID16341          FN16341     064-258-853   \n",
       "2    P1 - Tübingen            ID23075          FN23075     167-759-767   \n",
       "3    P1 - Tübingen            ID24107          FN23075     444-749-699   \n",
       "4    P1 - Tübingen            ID27288          FN27286     493-515-735   \n",
       "27   P1 - Tübingen             ATX553           ATX553             NaN   \n",
       "..             ...                ...              ...             ...   \n",
       "742     P7 - Paris  AAR-STR-CHA-314-1  AAR-STR-CHA-314             NaN   \n",
       "744     P7 - Paris  AAR-STR-CHA-314-2  AAR-STR-CHA-314             NaN   \n",
       "746     P7 - Paris  AAR-SAL-GOL-61-19              NaN             NaN   \n",
       "748     P7 - Paris   AAD-LYO-BUF-28-4              NaN             NaN   \n",
       "750     P7 - Paris  AAR-SAL-ROD-819-1              NaN             NaN   \n",
       "\n",
       "               ngs_database_id  main_phenotype  case_status  \\\n",
       "0                     TreatHSP  spastic ataxia  solved SACS   \n",
       "2                     TreatHSP  spastic ataxia  solved SACS   \n",
       "3                     TreatHSP  spastic ataxia  solved SACS   \n",
       "4                     TreatHSP  spastic ataxia  solved SACS   \n",
       "27                     Genesis          ataxia  solved SACS   \n",
       "..                         ...             ...          ...   \n",
       "742  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "744  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "746  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "748  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "750  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "\n",
       "                      id  gene  chrom  ...         prot_change genotype  \\\n",
       "0     P1_SACS_Tubingen_1  SACS     13  ...        p.Gly1275Ter      Het   \n",
       "2     P1_SACS_Tubingen_3  SACS     13  ...        p.Arg1575Trp      Hom   \n",
       "3     P1_SACS_Tubingen_4  SACS     13  ...        p.Arg1575Trp      Hom   \n",
       "4     P1_SACS_Tubingen_5  SACS     13  ...   p.Thr2388Argfs*10      Hom   \n",
       "27   P1_SACS_Tubingen_28  SACS     13  ...        p.Arg3903Ter      Hom   \n",
       "..                   ...   ...    ...  ...                 ...      ...   \n",
       "742     P7_SACS_Paris_84  SACS     13  ...         p.Asp202Gly      Het   \n",
       "744     P7_SACS_Paris_86  SACS     13  ...         p.Asp202Gly      Het   \n",
       "746     P7_SACS_Paris_88  SACS     13  ...  p.Trp144ValfsTer39      Het   \n",
       "748     P7_SACS_Paris_90  SACS     13  ...    p.Leu96ThrfsTer3      Het   \n",
       "750     P7_SACS_Paris_92  SACS     13  ...         p.Arg742Ter      Hom   \n",
       "\n",
       "      compound_het_id_s paxgene_availability pbmc_availability  \\\n",
       "0    P1_SACS_Tubingen_2                   no                no   \n",
       "2                   NaN                  yes                no   \n",
       "3                   NaN                   no                no   \n",
       "4                   NaN                   no                no   \n",
       "27                  NaN              unknown           unknown   \n",
       "..                  ...                  ...               ...   \n",
       "742    P7_SACS_Paris_85                   no               yes   \n",
       "744    P7_SACS_Paris_87                   no               yes   \n",
       "746    P7_SACS_Paris_89                   no               yes   \n",
       "748    P7_SACS_Paris_91                   no               yes   \n",
       "750                 NaN                   no               yes   \n",
       "\n",
       "    fibroblasts_availability comments   Variant_based_id tier1_is_solved  \\\n",
       "0                         no      NaN    13-23914246-C-A               y   \n",
       "2                         no      NaN    13-23913292-G-A               n   \n",
       "3                         no      NaN    13-23913292-G-A               n   \n",
       "4                         no      NaN  13-23910851-GGT-G               y   \n",
       "27                   unknown      NaN    13-23906308-G-A               y   \n",
       "..                       ...      ...                ...             ...   \n",
       "742                       no      NaN    13-23930146-T-C               n   \n",
       "744                       no      NaN    13-23930146-T-C               n   \n",
       "746                       no      NaN  13-23939332-AAA-A               y   \n",
       "748                       no      NaN   13-23942600-G-GT               n   \n",
       "750                       no      NaN    13-23915791-G-A               y   \n",
       "\n",
       "    is_tier2  \n",
       "0          n  \n",
       "2          n  \n",
       "3          n  \n",
       "4          n  \n",
       "27         n  \n",
       "..       ...  \n",
       "742        y  \n",
       "744        y  \n",
       "746        n  \n",
       "748        y  \n",
       "750        n  \n",
       "\n",
       "[170 rows x 25 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df_sacs[df_sacs[\"case_status\"]==\"solved SACS\"].drop_duplicates(subset=[\"submitter_id\", \"local_case_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submitter_id</th>\n",
       "      <th>local_case_id</th>\n",
       "      <th>local_family_id</th>\n",
       "      <th>prospax_case_id</th>\n",
       "      <th>ngs_database_id</th>\n",
       "      <th>main_phenotype</th>\n",
       "      <th>case_status</th>\n",
       "      <th>id</th>\n",
       "      <th>gene</th>\n",
       "      <th>chrom</th>\n",
       "      <th>...</th>\n",
       "      <th>cdna</th>\n",
       "      <th>prot_change</th>\n",
       "      <th>genotype</th>\n",
       "      <th>compound_het_id_s</th>\n",
       "      <th>paxgene_availability</th>\n",
       "      <th>pbmc_availability</th>\n",
       "      <th>fibroblasts_availability</th>\n",
       "      <th>comments</th>\n",
       "      <th>Variant_based_id</th>\n",
       "      <th>tier1_is_solved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID16341</td>\n",
       "      <td>FN16341</td>\n",
       "      <td>064-258-853</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_1</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.3769G&gt;T</td>\n",
       "      <td>p.Gly1275Ter</td>\n",
       "      <td>Het</td>\n",
       "      <td>P1_SACS_Tubingen_2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23914246-C-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ID27288</td>\n",
       "      <td>FN27286</td>\n",
       "      <td>493-515-735</td>\n",
       "      <td>TreatHSP</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_5</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.7162_7163del</td>\n",
       "      <td>p.Thr2388Argfs*10</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23910851-GGT-G</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>ATX553</td>\n",
       "      <td>ATX553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_28</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.11707C&gt;T</td>\n",
       "      <td>p.Arg3903Ter</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23906308-G-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>AAR-SAL-BOU-196</td>\n",
       "      <td>AAR-SAL-BOU-196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_29</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.2018dup</td>\n",
       "      <td>p.Asn673LysfsTer33</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23928732-A-AT</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>P1 - Tübingen</td>\n",
       "      <td>AAR-MAR-REB-111</td>\n",
       "      <td>AAR-MAR-REB-111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P1_SACS_Tubingen_40</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.12220G&gt;C</td>\n",
       "      <td>p.Ala4074Pro</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23905795-C-G</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-DIJ-REC-138-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_68</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.12973C&gt;T</td>\n",
       "      <td>p.Arg4325Ter</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_69</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23905042-G-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>FSP-SAL-MAR-1562-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_70</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.11914C&gt;T</td>\n",
       "      <td>p.Arg3972Ter</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_71</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23906101-G-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-SAL-DUC-225-8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_76</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.4933C&gt;T</td>\n",
       "      <td>p.Arg1645Ter</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_77</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23913082-G-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-SAL-GOL-61-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_88</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.429_430del</td>\n",
       "      <td>p.Trp144ValfsTer39</td>\n",
       "      <td>Het</td>\n",
       "      <td>P7_SACS_Paris_89</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23939332-AAA-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>P7 - Paris</td>\n",
       "      <td>AAR-SAL-ROD-819-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris diagnostic pipeline</td>\n",
       "      <td>spastic ataxia</td>\n",
       "      <td>solved SACS</td>\n",
       "      <td>P7_SACS_Paris_92</td>\n",
       "      <td>SACS</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>c.2224C&gt;T</td>\n",
       "      <td>p.Arg742Ter</td>\n",
       "      <td>Hom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13-23915791-G-A</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      submitter_id        local_case_id  local_family_id prospax_case_id  \\\n",
       "0    P1 - Tübingen              ID16341          FN16341     064-258-853   \n",
       "4    P1 - Tübingen              ID27288          FN27286     493-515-735   \n",
       "27   P1 - Tübingen               ATX553           ATX553             NaN   \n",
       "28   P1 - Tübingen      AAR-SAL-BOU-196  AAR-SAL-BOU-196             NaN   \n",
       "39   P1 - Tübingen      AAR-MAR-REB-111  AAR-MAR-REB-111             NaN   \n",
       "..             ...                  ...              ...             ...   \n",
       "728     P7 - Paris    AAR-DIJ-REC-138-4              NaN             NaN   \n",
       "730     P7 - Paris  FSP-SAL-MAR-1562-17              NaN             NaN   \n",
       "736     P7 - Paris    AAR-SAL-DUC-225-8              NaN             NaN   \n",
       "746     P7 - Paris    AAR-SAL-GOL-61-19              NaN             NaN   \n",
       "750     P7 - Paris    AAR-SAL-ROD-819-1              NaN             NaN   \n",
       "\n",
       "               ngs_database_id  main_phenotype  case_status  \\\n",
       "0                     TreatHSP  spastic ataxia  solved SACS   \n",
       "4                     TreatHSP  spastic ataxia  solved SACS   \n",
       "27                     Genesis          ataxia  solved SACS   \n",
       "28                     Genesis          ataxia  solved SACS   \n",
       "39                     Genesis          ataxia  solved SACS   \n",
       "..                         ...             ...          ...   \n",
       "728  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "730  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "736  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "746  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "750  Paris diagnostic pipeline  spastic ataxia  solved SACS   \n",
       "\n",
       "                      id  gene  chrom  ...            cdna  \\\n",
       "0     P1_SACS_Tubingen_1  SACS     13  ...       c.3769G>T   \n",
       "4     P1_SACS_Tubingen_5  SACS     13  ...  c.7162_7163del   \n",
       "27   P1_SACS_Tubingen_28  SACS     13  ...      c.11707C>T   \n",
       "28   P1_SACS_Tubingen_29  SACS     13  ...       c.2018dup   \n",
       "39   P1_SACS_Tubingen_40  SACS     13  ...      c.12220G>C   \n",
       "..                   ...   ...    ...  ...             ...   \n",
       "728     P7_SACS_Paris_68  SACS     13  ...      c.12973C>T   \n",
       "730     P7_SACS_Paris_70  SACS     13  ...      c.11914C>T   \n",
       "736     P7_SACS_Paris_76  SACS     13  ...       c.4933C>T   \n",
       "746     P7_SACS_Paris_88  SACS     13  ...    c.429_430del   \n",
       "750     P7_SACS_Paris_92  SACS     13  ...       c.2224C>T   \n",
       "\n",
       "            prot_change genotype   compound_het_id_s paxgene_availability  \\\n",
       "0          p.Gly1275Ter      Het  P1_SACS_Tubingen_2                   no   \n",
       "4     p.Thr2388Argfs*10      Hom                 NaN                   no   \n",
       "27         p.Arg3903Ter      Hom                 NaN              unknown   \n",
       "28   p.Asn673LysfsTer33      Hom                 NaN              unknown   \n",
       "39         p.Ala4074Pro      Hom                 NaN              unknown   \n",
       "..                  ...      ...                 ...                  ...   \n",
       "728        p.Arg4325Ter      Het    P7_SACS_Paris_69                   no   \n",
       "730        p.Arg3972Ter      Het    P7_SACS_Paris_71                   no   \n",
       "736        p.Arg1645Ter      Het    P7_SACS_Paris_77                   no   \n",
       "746  p.Trp144ValfsTer39      Het    P7_SACS_Paris_89                   no   \n",
       "750         p.Arg742Ter      Hom                 NaN                   no   \n",
       "\n",
       "    pbmc_availability fibroblasts_availability comments   Variant_based_id  \\\n",
       "0                  no                       no      NaN    13-23914246-C-A   \n",
       "4                  no                       no      NaN  13-23910851-GGT-G   \n",
       "27            unknown                  unknown      NaN    13-23906308-G-A   \n",
       "28            unknown                  unknown      NaN   13-23928732-A-AT   \n",
       "39            unknown                  unknown      NaN    13-23905795-C-G   \n",
       "..                ...                      ...      ...                ...   \n",
       "728               yes                       no      NaN    13-23905042-G-A   \n",
       "730               yes                       no      NaN    13-23906101-G-A   \n",
       "736               yes                       no      NaN    13-23913082-G-A   \n",
       "746               yes                       no      NaN  13-23939332-AAA-A   \n",
       "750               yes                       no      NaN    13-23915791-G-A   \n",
       "\n",
       "    tier1_is_solved  \n",
       "0                 y  \n",
       "4                 y  \n",
       "27                y  \n",
       "28                y  \n",
       "39                y  \n",
       "..              ...  \n",
       "728               y  \n",
       "730               y  \n",
       "736               y  \n",
       "746               y  \n",
       "750               y  \n",
       "\n",
       "[131 rows x 24 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solved[df_solved[\"case_status\"]==\"solved SACS\"].drop_duplicates(subset=[\"submitter_id\", \"local_case_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 25)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sacs.drop_duplicates(subset=[\"submitter_id\", \"local_case_id\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_all_words(alphabet, k, base=\"\"):\n",
    "    n = len(alphabet)\n",
    "    recursive_function(alphabet, n, k, base)\n",
    "\n",
    "def recursive_function(alphabet, length_alphabet, k, base):\n",
    "    if k==0:\n",
    "        print(base)\n",
    "        return \n",
    "    for i in range(length_alphabet):\n",
    "        new_word = base + alphabet[i]\n",
    "        # print(new_word)\n",
    "        recursive_function(alphabet, length_alphabet, k-1, new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "aa\n",
      "ab\n",
      "ac\n",
      "ba\n",
      "bb\n",
      "bc\n",
      "ca\n",
      "cb\n",
      "cc\n",
      "aaa\n",
      "aab\n",
      "aac\n",
      "aba\n",
      "abb\n",
      "abc\n",
      "aca\n",
      "acb\n",
      "acc\n",
      "baa\n",
      "bab\n",
      "bac\n",
      "bba\n",
      "bbb\n",
      "bbc\n",
      "bca\n",
      "bcb\n",
      "bcc\n",
      "caa\n",
      "cab\n",
      "cac\n",
      "cba\n",
      "cbb\n",
      "cbc\n",
      "cca\n",
      "ccb\n",
      "ccc\n"
     ]
    }
   ],
   "source": [
    "alpha = [\"a\", \"b\", \"c\"]\n",
    "l = 2\n",
    "for i in range(len(alpha)):\n",
    "    return_all_words(alpha, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutations(l, n=-1, com_list=[]):\n",
    "    # basis steps\n",
    "    if n < 0:\n",
    "        n = len(l)\n",
    "    # base case of the recursive - when we reach n==0\n",
    "    # return final state\n",
    "    if n == 0:\n",
    "        # at every call of function we decrement n \n",
    "        # if n = 0 it stops calling recursively the function\n",
    "        return com_list\n",
    "    \n",
    "    # Recursive block\n",
    "    # Decompose the original problem into simpler instances of the same problem\n",
    "    sub_list=[]\n",
    "    # very first recursive call\n",
    "    # populate the list with all length=1 word\n",
    "    if len(com_list)==0:\n",
    "        for item in l:\n",
    "            sub_list.append(item)\n",
    "    else:\n",
    "        # iterate on the set of letters\n",
    "        for item in l:\n",
    "            # uses the elements generated by the last recursive call\n",
    "            for item2 in com_list:\n",
    "                # concatenate the all words from last recursive call --> to the current letter from set\n",
    "                sub_list.append(f\"{item}{item2}\")\n",
    "    # at each return of the function the sublist is added to com_list\n",
    "    # each call of the function generate words of ONE given length \n",
    "    return com_list + permutations(l, n-1, sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'aa',\n",
       " 'ab',\n",
       " 'ac',\n",
       " 'ba',\n",
       " 'bb',\n",
       " 'bc',\n",
       " 'ca',\n",
       " 'cb',\n",
       " 'cc',\n",
       " 'aaa',\n",
       " 'aab',\n",
       " 'aac',\n",
       " 'aba',\n",
       " 'abb',\n",
       " 'abc',\n",
       " 'aca',\n",
       " 'acb',\n",
       " 'acc',\n",
       " 'baa',\n",
       " 'bab',\n",
       " 'bac',\n",
       " 'bba',\n",
       " 'bbb',\n",
       " 'bbc',\n",
       " 'bca',\n",
       " 'bcb',\n",
       " 'bcc',\n",
       " 'caa',\n",
       " 'cab',\n",
       " 'cac',\n",
       " 'cba',\n",
       " 'cbb',\n",
       " 'cbc',\n",
       " 'cca',\n",
       " 'ccb',\n",
       " 'ccc']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3 program to print all\n",
    "# possible strings of length k\n",
    "\t\n",
    "# The method that prints all\n",
    "# possible strings of length k.\n",
    "# It is mainly a wrapper over\n",
    "# recursive function printAllKLengthRec()\n",
    "def printAllKLength(set, k):\n",
    "\n",
    "\tn = len(set)\n",
    "\tprintAllKLengthRec(set, \"\", n, k)\n",
    "\n",
    "# The main recursive method\n",
    "# to print all possible\n",
    "# strings of length k\n",
    "def printAllKLengthRec(set, prefix, n, k):\n",
    "\t\n",
    "\t# Base case: k is 0,\n",
    "\t# print prefix\n",
    "\tif (k == 0) :\n",
    "\t\tprint(prefix)\n",
    "\t\treturn\n",
    "\n",
    "\t# One by one add all characters\n",
    "\t# from set and recursively\n",
    "\t# call for k equals to k-1\n",
    "\tfor i in range(n):\n",
    "\n",
    "\t\t# Next character of input added\n",
    "\t\tnewPrefix = prefix + set[i]\n",
    "\t\t\n",
    "\t\t# k is decreased, because\n",
    "\t\t# we have added a new character\n",
    "\t\tprintAllKLengthRec(set, newPrefix, n, k - 1)\n",
    "\n",
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\tprint(\"First Test\")\n",
    "\tset1 = ['a', 'b']\n",
    "\tk = 3\n",
    "\tprintAllKLength(set1, k)\n",
    "\t\n",
    "\tprint(\"\\nSecond Test\")\n",
    "\tset2 = ['a', 'b', 'c', 'd']\n",
    "\tk = 1\n",
    "\tprintAllKLength(set2, k)\n",
    "\n",
    "# This code is contributed\n",
    "# by ChitraNayal\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
